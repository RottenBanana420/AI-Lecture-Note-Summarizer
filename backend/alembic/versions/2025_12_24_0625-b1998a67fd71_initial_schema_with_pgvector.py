"""initial_schema_with_pgvector

Revision ID: b1998a67fd71
Revises: 
Create Date: 2025-12-24 06:25:23.793353

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
from pgvector.sqlalchemy import Vector
import sys
from pathlib import Path

# Add alembic directory to path to import migration utilities
alembic_dir = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(alembic_dir))
from migration_utils import create_pgvector_extension, create_hnsw_index, drop_index


# revision identifiers, used by Alembic.
revision: str = 'b1998a67fd71'
down_revision: Union[str, Sequence[str], None] = None
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    """Upgrade schema."""
    # Enable pgvector extension for vector similarity search
    create_pgvector_extension()
    
    # ### commands auto generated by Alembic - please adjust! ###
    op.create_table('users',
    sa.Column('username', sa.String(length=50), nullable=False, comment='Unique username for authentication'),
    sa.Column('email', sa.String(length=255), nullable=False, comment="User's email address"),
    sa.Column('hashed_password', sa.String(length=255), nullable=False, comment='Bcrypt hashed password'),
    sa.Column('full_name', sa.String(length=100), nullable=True, comment="User's full name"),
    sa.Column('is_active', sa.String(length=1), server_default='1', nullable=False, comment='Whether the user account is active'),
    sa.Column('is_superuser', sa.String(length=1), server_default='0', nullable=False, comment='Whether the user has admin privileges'),
    sa.Column('id', sa.Integer(), autoincrement=True, nullable=False),
    sa.Column('created_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=False, comment='Timestamp when the record was created'),
    sa.Column('updated_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=False, comment='Timestamp when the record was last updated'),
    sa.PrimaryKeyConstraint('id'),
    comment='Users table for authentication and user management'
    )
    op.create_index(op.f('ix_users_email'), 'users', ['email'], unique=True)
    op.create_index(op.f('ix_users_id'), 'users', ['id'], unique=False)
    op.create_index(op.f('ix_users_username'), 'users', ['username'], unique=True)
    op.create_index('ix_users_username_email', 'users', ['username', 'email'], unique=False)
    op.create_table('documents',
    sa.Column('title', sa.String(length=255), nullable=False, comment='Document title'),
    sa.Column('original_filename', sa.String(length=255), nullable=False, comment='Original filename of the uploaded document'),
    sa.Column('file_size', sa.BigInteger(), nullable=False, comment='File size in bytes'),
    sa.Column('mime_type', sa.String(length=100), nullable=False, comment='MIME type of the document'),
    sa.Column('file_path', sa.String(length=500), nullable=False, comment='Storage path or key for the document file'),
    sa.Column('processing_status', sa.Enum('pending', 'processing', 'completed', 'failed', name='processing_status_enum'), server_default='pending', nullable=False, comment='Current processing status of the document'),
    sa.Column('user_id', sa.Integer(), nullable=True, comment='ID of the user who uploaded the document'),
    sa.Column('uploaded_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=False, comment='Timestamp when the document was uploaded'),
    sa.Column('id', sa.Integer(), autoincrement=True, nullable=False),
    sa.Column('created_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=False, comment='Timestamp when the record was created'),
    sa.Column('updated_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=False, comment='Timestamp when the record was last updated'),
    sa.ForeignKeyConstraint(['user_id'], ['users.id'], ondelete='SET NULL'),
    sa.PrimaryKeyConstraint('id'),
    sa.UniqueConstraint('file_path'),
    comment='Documents table for storing uploaded document metadata'
    )
    op.create_index(op.f('ix_documents_id'), 'documents', ['id'], unique=False)
    op.create_index(op.f('ix_documents_processing_status'), 'documents', ['processing_status'], unique=False)
    op.create_index(op.f('ix_documents_title'), 'documents', ['title'], unique=False)
    op.create_index(op.f('ix_documents_uploaded_at'), 'documents', ['uploaded_at'], unique=False)
    op.create_index(op.f('ix_documents_user_id'), 'documents', ['user_id'], unique=False)
    op.create_index('ix_documents_user_status', 'documents', ['user_id', 'processing_status'], unique=False)
    op.create_table('note_chunks',
    sa.Column('id', sa.Integer(), autoincrement=True, nullable=False),
    sa.Column('document_id', sa.Integer(), nullable=False, comment='ID of the document this chunk belongs to'),
    sa.Column('chunk_text', sa.Text(), nullable=False, comment='The text content of this chunk'),
    sa.Column('chunk_index', sa.Integer(), nullable=False, comment='Position of this chunk within the document (0-indexed)'),
    sa.Column('embedding', Vector(1536), nullable=True, comment='Vector embedding of the chunk text for similarity search'),
    sa.Column('character_count', sa.Integer(), nullable=False, comment='Number of characters in the chunk'),
    sa.Column('token_count', sa.Integer(), nullable=True, comment='Approximate number of tokens in the chunk'),
    sa.Column('chunk_metadata', sa.JSON(), nullable=True, comment='Additional metadata about the chunk (e.g., page number, section)'),
    sa.Column('created_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=False, comment='Timestamp when the chunk was created'),
    sa.ForeignKeyConstraint(['document_id'], ['documents.id'], ondelete='CASCADE'),
    sa.PrimaryKeyConstraint('id'),
    comment='Note chunks table for storing document chunks with vector embeddings'
    )
    op.create_index(op.f('ix_note_chunks_document_id'), 'note_chunks', ['document_id'], unique=False)
    op.create_index('ix_note_chunks_document_index', 'note_chunks', ['document_id', 'chunk_index'], unique=False)
    op.create_index(op.f('ix_note_chunks_id'), 'note_chunks', ['id'], unique=False)
    op.create_table('summaries',
    sa.Column('id', sa.Integer(), autoincrement=True, nullable=False),
    sa.Column('document_id', sa.Integer(), nullable=False, comment='ID of the document this summary belongs to'),
    sa.Column('summary_text', sa.Text(), nullable=False, comment='The generated summary text'),
    sa.Column('summary_type', sa.Enum('extractive', 'abstractive', name='summary_type_enum'), nullable=False, comment='Type of summary generation method used'),
    sa.Column('processing_duration', sa.Float(), nullable=True, comment='Time taken to generate the summary in seconds'),
    sa.Column('summary_metadata', sa.JSON(), nullable=True, comment='Additional metadata about the summarization process'),
    sa.Column('generated_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=False, comment='Timestamp when the summary was generated'),
    sa.ForeignKeyConstraint(['document_id'], ['documents.id'], ondelete='CASCADE'),
    sa.PrimaryKeyConstraint('id'),
    comment='Summaries table for storing AI-generated document summaries'
    )
    op.create_index(op.f('ix_summaries_document_id'), 'summaries', ['document_id'], unique=False)
    op.create_index('ix_summaries_document_type', 'summaries', ['document_id', 'summary_type'], unique=False)
    op.create_index(op.f('ix_summaries_generated_at'), 'summaries', ['generated_at'], unique=False)
    op.create_index(op.f('ix_summaries_id'), 'summaries', ['id'], unique=False)
    op.create_index(op.f('ix_summaries_summary_type'), 'summaries', ['summary_type'], unique=False)
    # ### end Alembic commands ###
    
    # Create HNSW index for vector similarity search on note_chunks.embedding
    # HNSW (Hierarchical Navigable Small World) provides fast approximate nearest neighbor search
    # Parameters:
    #   m=16: Number of connections per layer (higher = better recall, more memory)
    #   ef_construction=64: Size of dynamic candidate list (higher = better quality, slower build)
    #   vector_cosine_ops: Use cosine distance metric (1 - cosine similarity)
    create_hnsw_index(
        index_name='ix_note_chunks_embedding_hnsw',
        table_name='note_chunks',
        column_name='embedding',
        m=16,
        ef_construction=64,
        distance_metric='vector_cosine_ops'
    )


def downgrade() -> None:
    """Downgrade schema."""
    # Drop HNSW index first
    drop_index('ix_note_chunks_embedding_hnsw')
    
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_index(op.f('ix_summaries_summary_type'), table_name='summaries')
    op.drop_index(op.f('ix_summaries_id'), table_name='summaries')
    op.drop_index(op.f('ix_summaries_generated_at'), table_name='summaries')
    op.drop_index('ix_summaries_document_type', table_name='summaries')
    op.drop_index(op.f('ix_summaries_document_id'), table_name='summaries')
    op.drop_table('summaries')
    op.drop_index(op.f('ix_note_chunks_id'), table_name='note_chunks')
    op.drop_index('ix_note_chunks_document_index', table_name='note_chunks')
    op.drop_index(op.f('ix_note_chunks_document_id'), table_name='note_chunks')
    op.drop_table('note_chunks')
    op.drop_index('ix_documents_user_status', table_name='documents')
    op.drop_index(op.f('ix_documents_user_id'), table_name='documents')
    op.drop_index(op.f('ix_documents_uploaded_at'), table_name='documents')
    op.drop_index(op.f('ix_documents_title'), table_name='documents')
    op.drop_index(op.f('ix_documents_processing_status'), table_name='documents')
    op.drop_index(op.f('ix_documents_id'), table_name='documents')
    op.drop_table('documents')
    op.drop_index('ix_users_username_email', table_name='users')
    op.drop_index(op.f('ix_users_username'), table_name='users')
    op.drop_index(op.f('ix_users_id'), table_name='users')
    op.drop_index(op.f('ix_users_email'), table_name='users')
    op.drop_table('users')
    # ### end Alembic commands ###
    
    # Drop enum types (must be done after dropping tables that use them)
    op.execute('DROP TYPE IF EXISTS summary_type_enum')
    op.execute('DROP TYPE IF EXISTS processing_status_enum')
